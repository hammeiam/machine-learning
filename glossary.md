weight: A value indicating the extent to which an input activation is considered by a neuron.
Can be positive or negative with no bounds.

bias: A value connected to a layer of neurons through a weight (just like an input activation)
  which serves to improve the accuracy of the activation function by shifting the function
  to the left or right on a graph. More info:
  http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks

activation: A value from a neuron (input activation, output activation)

activation function: A function that turns the preactivation value into a normalized value,
  usually between 0,1 or -1,1 depending on the function used.

preactivation: In a neuron, the sum of that layer's bias and the product of an input times its corresponding weight for all inputs. (bias + E(i * w))

cost function:

gradient descent:

backpropogation: 
